##
# 一，使用场景

1. 情感分析:正负
2. 领域分析:一级二级，粗分类，细分类
3. 等等

# 二，基本流程

1. 定义类别:根据任务的要求和数据的质量选取合适的类别
2. 收集数据:自行标注/往上爬取
3. 模型训练:多种模型根据有无GPU，需要的指标类型，指标最低分数等决定如何选取
4. 预测:根据是否通过网络调用判断是否需要封装成web接口

# 三，可选的模型

## 1.机器学习方法

朴素贝叶斯、SVM、树集成模型等等

优点:

1. 训练和预测都相对比较快
2. 可先做baseline跑通联调流程

缺点:

1. 泛化能力较弱
2. 指标分数大多数没有调好的深度学习模型高

## 2.深度学习方法

### 1.fasttext[1607.01759.pdf (arxiv.org)](https://arxiv.org/pdf/1607.01759.pdf)

#### 方法

将每个词的Embedding取平均过一个全连接层直接预测类别概率

#### 优缺点

优点:

1. 快，尤其相比预训练模型快上几个数量级
2. 实现简单，而且效果不错

缺点:

1. 无法超越复杂模型的评测分数
2. 无法完成复杂NLP任务，机器翻译等

### 2.TextRNN[1605.05101.pdf (arxiv.org)](https://arxiv.org/pdf/1605.05101.pdf)

#### 方法

将每个词的Embedding过一个RNN类(RNN,GRU,LSTM,1层,2层等)的模型作为输出，然后接一个全连接层预测分类概率

### 3.TextCNN[1408.5882.pdf (arxiv.org)](https://arxiv.org/pdf/1408.5882.pdf)

#### 方法

将每个词的Embedding过多个1D卷积作为输出，然后接最大池化取出多个特征，然后接一个全连接层预测分类概率

### 4.GatedCNN[Language Modeling with Gated Convolutional Networks (arxiv.org)](https://arxiv.org/pdf/1612.08083.pdf)

#### 方法

通过一个门控CNN乘上原CNN的输出实现门机制

### 5.TextRCNN[Recurrent Convolutional Neural Networks for Text Classification (ia.ac.cn)](http://www.nlpr.ia.ac.cn/cip/~liukang/liukangPageFile/Recurrent%20Convolutional%20Neural%20Networks%20for%20Text%20Classification.pdf)

#### 方法

双向RNN类模型与原词向量拼接后过一个最大池化再过一个全连接层输出概率

### 6.Bert[1810.04805v2.pdf (arxiv.org)](https://arxiv.org/pdf/1810.04805v2.pdf)

#### 方法

通过无标注的数据训练一个通用的NLP基础模型给下游任务做fine-tune(微调)

#### 优缺点

优点:

1. 效果好，指标方面，能一定程度预防样本不均衡等

缺点:

1. 慢，相对于浅层网络训练慢，预测也慢
2. 大，模型的参数很多，储存也占空间

# 四，工程问题

## 1.数据稀疏

> 有类别的数据很少

### 数据上

1. 找数据:标注更多数据
2. 数据增强:通过各种方法生成更多数据

### 模型上

1. 预训练:通过预训练模型本身在训练时就接触了大量的数据基础来缓解数据稀疏问题
2. 添加BN\LN\dropout等减少过拟合的结构

## 2.标签不均衡

> 简单说就是某一(些)类别数据多了或者少了，让模型学习的时候有偏向

如果数据少了，上面数据稀疏的方法同样也可以用

### 采样上：

1. 过采样与降采样:重复采样少的类别数据，少采样多的数据

## 3.多标签任务

> 多分类任务其实是特殊的多标签任务，也就是每个数据只有一类标签，但有时一个数据会属于两个类别(标签)，就成了多标签任务

### 1.one to rest

将多标签问题转换为多个二分类任务训练多个模型进行独立预测

### 2.BCEloss

用BCEloss直接进行多标签训练

# 五，经验总结

上线首选Fasttext做baseline

不上线直接上预训练模型
