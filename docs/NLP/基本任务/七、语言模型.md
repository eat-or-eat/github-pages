##
# 一，什么是语言模型

用来计算一个句子的成句概率的模型

举例：

今天天气晴朗，我心情也不错 ->合理

今天心情晴朗，我天气也不过 ->不合理

# 二，统计语言模型

## 1.怎么来的：N-gram

思想：假设每个词出现的可能性都跟出现在这个词之前的**上文**(为后面的双向模型发展埋下伏笔)相关

原来的公式：$P(S) = P(W_1,W_2,...,W_n) = P(W_1)P(W_2|W_1)...P(Wn|W_1W_2,...,W_{n-1})$

但这个公式在求解的时候有两个问题，一个是条件概率不好算(参数空间过大)，一个是训练数据集会对验证集出现过拟合的情况（数据稀疏严重）

> 解决条件概率不好算问题

引入马尔可夫假设来解决：随意一个词出现的概率只与它前面出现的有限的一个或者几个词有关

举例：与前一个有关就是bigram，与前两个有关就是trigram

bigram:$P(S) = P(W_1,W_2,...,W_n) = P(W_1)P(W_2|W_1)...P(Wn|W_{n-1})$

> 训练数据集会对验证集出现过拟合

词方面：对于没有出现过的词概率用拉普拉斯平滑:$\frac{num(word)+1}{num(totalword)+V}$

gram方面：对于没有出现过的gram可以通过设定回退概率找前一级gram，没有$P(W_3|W_1W_2)$找$P(W_3|W_2)$ 然后乘上一个(0,1)的回退值，最后问题会落到没出现过的词或者字上

## 2.怎么评价：PPL（perplexity）

思想：通过一种计算方式设计一个指标与成句概率成反比，值越小，则代表越可能成句

PPL=$P(S)^{-\frac{1}{N}}$

注意，不同的模型之间PPL没有可比性，因为本身模型的训练数据就不一样

## 3.优缺点

优点：

1. 相对于神经网络模型的预测会快很多

缺点：

1. 泛化能力差，因为gram不可能考虑到所有的可能，模型的好坏跟训练数据和应用领域之间有很大的关系
2. 训练数据越大，模型越大，因为模型的本质就是储存n-gram的概率
3. 没有考虑词义和语序的关系，无法处理长句子的依赖语义

# 三，神经网络语言模型

## 1.NNLM

### 怎么来的

> 原论文:[bengio03a.dvi (jmlr.org)](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)

模拟条件概率作为神经网络的目标函数进行训练

举例：

一条训练样本：今天 天气 不错 我们 去 吃 北京烤鸭

input:今天 天气 不错 我们 去 吃 ;output:北京烤鸭

流程：char->index->embedding->model->softmax(probability)->loss->change Ws

### 怎么评价

用模型自带的loss进行评价，和PPL同样，不同训练数据的语言模型之间的loss没有可比性

### 优缺点

优点：

1. 整体上泛化能力相对于统计语言模型好，因为不同训练数据、不同模型结构、不同应用方式可能会有不同的出入
2. 能对长距离的句子更有效
3. 能获表示词的意思

缺点：

1. 预测速度慢，越大的模型预测越慢，尤其是RNN类不能并行计算的模型
2. 训练时需要调参试错

## 2.word2vec

[主要介绍CBOW,Skip-gram,Glove三种模型](./六、word2vec.md)

## 3.Elmo

> 原论文:[1802.05365.pdf (arxiv.org)](https://arxiv.org/pdf/1802.05365.pdf)

### 特点

1. 通过双向LSEM对输入语句进行预测
2. 下游任务中将elmo的词向量进行原词向量拼接使用
3. 预训练任务是一种通用任务的提升效果的方法

## 缺点

1. bilstm只是单独的用了正向和反向进行拼接，没有真正的让模型对整个句子上下文的融合学习
2. lstm模型可以替换成transformer进行效果提升

## 4.GPT

> 原论文:[language_understanding_paper.pdf](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)

### 特点

1. 将lstm替换成了transformer
2. 仍坚持自回归的方法进行预训练

## 5.Bert

> 原论文:[1810.04805v2.pdf (arxiv.org)](https://arxiv.org/pdf/1810.04805v2.pdf)

### 特点

1. 双向transformer编码，因为使用了自编码的方式进行预训练(MLM)
2. NSP任务，不过在后面被指明太简单效果不大，可以用SOP任务替换
3. 输入的Embedding有三种方式，延伸出后面的各种Embedding方式


# 四，其他经典预训练模型

- 知识蒸馏:TinyBert,DistilBert
- 参数共享:ALBert
- 特定语言:Bert-wwm
- 更猛的数据和训练:RoBert
- GPT之后:GPT2,GPT3
