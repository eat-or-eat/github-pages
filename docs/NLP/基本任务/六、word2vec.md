##
# 一，为什么要用word2vec

问题: 传统的one-hot方法当词越来越多的时候，one-hot向量就会越来越长，而且很稀疏，无法词和词之间没有相关性或者时序语义的意思

解决问题的思路： 使用向量来代表某个字\词\句子

# 二，怎么获取

## 1.基于语言模型

1. NNLM中进获得的look up table
2. 自编码的Bert等模型
3. 自回归的GPT等模型



## 2.基于窗口

### 1.Skip-gram

方法：通过窗口中间词来预测周围词

### 2.CBOW

方法：通过窗口周围词来预测中间词

### 3.训练时的两个典型问题以及当时解决办法

> 输出层字符太多导致one-hot的维度灾难

通过对结果构建huffman树来进行训练，减少最后的计算量

> 输出层计算量太大

通过负采样的方式选择需要更新的节点，通过使用sigmoid代替softmax将输出值映射到(0,1)范围内

## 3.基于共现矩阵

思想：通过共现矩阵计算共现概率比，然后用共现概率比来做为训练目标训练词向量
